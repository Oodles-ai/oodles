{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we train a model using the orientation data of people during fitness exercises. The model tries to predict whether the person is in a vertical or a horizontal position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import subprocess\n",
    "import zipfile\n",
    "import numpy as np\n",
    "\n",
    "from oodles import Framework\n",
    "from oodles import Signal\n",
    "from oodles import monitor\n",
    "from oodles import ModelSignal, AnnotationMethod, Anomaly, DataDriftAlgo\n",
    "\n",
    "from dataset import input_to_dataset_transformation, read_json, write_json, KpsDataset\n",
    "from pushup_signal import pushup_signal\n",
    "from contextlib import redirect_stdout\n",
    "\n",
    "import tensorflow as tf\n",
    "import joblib\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download dataset from remote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_dir = \"data\"\n",
    "remote_url = \"https://oodles-dev-training-data.s3.amazonaws.com/data.zip\"\n",
    "orig_training_file = 'data/training_data.json'\n",
    "if not os.path.exists(data_dir):\n",
    "    try:\n",
    "        file_downloaded_ok = subprocess.check_output(\"wget \" + remote_url, shell=True)\n",
    "    except:\n",
    "        print(\"Could not load training data\")\n",
    "    with zipfile.ZipFile(\"data.zip\", 'r') as zip_ref:\n",
    "        zip_ref.extractall(\"./\")\n",
    "\n",
    "    full_training_data = read_json(orig_training_file)\n",
    "    np.random.seed(1)\n",
    "    np.random.shuffle(full_training_data)\n",
    "    reduced_training_data = full_training_data[0:1000]\n",
    "    write_json(orig_training_file, reduced_training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_world_test_cases = 'data/real_world_testing_data.json'\n",
    "golden_testing_file = 'data/golden_testing_data.json'\n",
    "annotation_args = {'master_file': 'data/master_annotation_data.json'}\n",
    "\n",
    "# Defining the egde-case signal\n",
    "pushup_edge_case = Signal(\"Pushup\", pushup_signal)\n",
    "inference_batch_size = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training with Logistic Regression (LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on:  data/training_data.json  which has  1000  data-points\n",
      "Model saved at:  trained_models_lr/version_0\n"
     ]
    }
   ],
   "source": [
    "from model_logistic_regression import get_accuracy_lr, train_model_lr\n",
    "train_model_lr('data/training_data.json', 'version_0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we evaluate the model on our golden testing dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating on  15731  data-points\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8586231008836056"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_accuracy_lr(golden_testing_file, 'version_0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that the testing accuracy of the model is quite low. On investigating further, we realize that it is because all pushup signals are being classified as \"vertical\" orientation. Next, we will define the oodles config with edge-case check for Pushup signals and also pass our training and evaluation arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = {\n",
    "    # Define your signal to identify edge cases\n",
    "    \"checks\": [{\n",
    "        'type': Anomaly.EDGE_CASE, \n",
    "        \"signal_formulae\": pushup_edge_case\n",
    "    }],\n",
    "    \"data_identifier\": \"id\",\n",
    "    \"batch_size\": inference_batch_size,\n",
    "\n",
    "    # Connect training pipeline to annotate data and retrain the model\n",
    "    \"training_args\": {\n",
    "        \"data_transformation_func\": input_to_dataset_transformation,  \n",
    "        \"annotation_method\": {\"method\": AnnotationMethod.MASTER_FILE, \"args\": annotation_args}, \n",
    "        \"training_func\": train_model_lr, \n",
    "        \"fold_name\": 'oodles_smart_data',  \n",
    "        \"orig_training_file\": orig_training_file,  \n",
    "    },\n",
    "\n",
    "    # Connect evaluation pipeline to test retrained model against original model\n",
    "    \"evaluation_args\": {\n",
    "        \"inference_func\": get_accuracy_lr,\n",
    "        \"golden_testing_dataset\": golden_testing_file,\n",
    "        \"metrics_to_check\": ['accuracy']\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "framework_lr = Framework(cfg)\n",
    "\n",
    "@monitor(framework_lr)\n",
    "def model_predict(model, inputs):\n",
    "    return model.predict(inputs['data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50  edge-cases collected out of  76  inferred samples\n",
      "100  edge-cases collected out of  145  inferred samples\n",
      "150  edge-cases collected out of  224  inferred samples\n",
      "200  edge-cases collected out of  296  inferred samples\n",
      "250  edge-cases collected out of  367  inferred samples\n",
      "Kicking off re-training\n",
      "251 data-points selected out of 371\n",
      "Training on:  oodles_smart_data/1/training_dataset.json  which has  2255  data-points\n",
      "Model saved at:  trained_models_lr/version_1\n",
      "Model retraining done...\n",
      "Generating comparison report...\n",
      "Training on:  data/training_data.json  which has  1000  data-points\n",
      "Trained model exists. Skipping training again.\n",
      "Evaluating on  15731  data-points\n",
      "Evaluating on  15731  data-points\n",
      "---------------------------------------------\n",
      "---------------------------------------------\n",
      "Old model accuracy:  0.8586231008836056\n",
      "Retrained model accuracy (ie 251 smartly collected data-points added):  0.94386879410082\n",
      "---------------------------------------------\n",
      "---------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "testing_dataset = KpsDataset(real_world_test_cases, normalization=True)\n",
    "X_test, y_test, id = testing_dataset.load_x_y_from_data()\n",
    "pred_classes = []\n",
    "model = joblib.load(\"trained_models_lr/\" + 'version_0')\n",
    "for i in range(int(np.ceil(len(X_test)/inference_batch_size))):\n",
    "\n",
    "    elem = X_test[i*inference_batch_size:min((i+1)*inference_batch_size,len(X_test))]\n",
    "    ids = id[i*inference_batch_size:min((i+1)*inference_batch_size,len(X_test))]\n",
    "\n",
    "    # Do model prediction\n",
    "    inputs = {\"data\": elem, \"id\": ids}\n",
    "    preds, idens = model_predict(model, inputs)\n",
    "\n",
    "    # Attach Ground Truth\n",
    "    gts = y_test[i*inference_batch_size:min((i+1)*inference_batch_size,len(X_test))]\n",
    "    framework_lr.attach_ground_truth({'id': idens, 'gt': gts})\n",
    "\n",
    "    if framework_lr.version > 1:\n",
    "        # Retrain only once\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the comparison report above, we can see how oodles improved the model performance by detecting edge-cases and retraining the model under-the-hood. Further, Oodles is agnostic to the model type and training functions. To illustrate this, we again train our orientation classification model, but this time with Deep Neural Networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training using Deep Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on:  data/training_data.json  which has  1000  data-points\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-13 14:02:37.803481: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "3/3 [==============================] - 1s 161ms/step - loss: 136.7002 - binary_accuracy: 0.2826\n",
      "Epoch 2/10\n",
      "3/3 [==============================] - 1s 181ms/step - loss: 129.5490 - binary_accuracy: 0.2812\n",
      "Epoch 3/10\n",
      "3/3 [==============================] - 1s 170ms/step - loss: 120.7798 - binary_accuracy: 0.3034\n",
      "Epoch 4/10\n",
      "3/3 [==============================] - 1s 162ms/step - loss: 113.0427 - binary_accuracy: 0.2839\n",
      "Epoch 5/10\n",
      "3/3 [==============================] - 1s 167ms/step - loss: 101.4785 - binary_accuracy: 0.3477\n",
      "Epoch 6/10\n",
      "3/3 [==============================] - 1s 166ms/step - loss: 92.1099 - binary_accuracy: 0.3008\n",
      "Epoch 7/10\n",
      "3/3 [==============================] - 1s 166ms/step - loss: 90.2377 - binary_accuracy: 0.3190\n",
      "Epoch 8/10\n",
      "3/3 [==============================] - 1s 169ms/step - loss: 77.1374 - binary_accuracy: 0.3581\n",
      "Epoch 9/10\n",
      "3/3 [==============================] - 1s 185ms/step - loss: 73.6066 - binary_accuracy: 0.3659\n",
      "Epoch 10/10\n",
      "3/3 [==============================] - 1s 180ms/step - loss: 69.8488 - binary_accuracy: 0.3711\n",
      "INFO:tensorflow:Assets written to: trained_models_dnn/version_0/assets\n",
      "Model saved at:  trained_models_dnn/version_0\n"
     ]
    }
   ],
   "source": [
    "from model_dnn import get_accuracy_dnn, train_model_dnn\n",
    "train_model_dnn('data/training_data.json', 'version_0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we get the model accuracy on testing dataset, which is again low due to misclassification of Pushup signals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating on  15731  data-points\n",
      "492/492 [==============================] - 0s 498us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.22382556735109022"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_accuracy_dnn(golden_testing_file, 'version_0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Update the Oodles config with new training workflows and checks. Let's also add a check for edge-cases when model confidence is low (because why not!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Whenever model confidence is <0.8, identify it as an edge-case \n",
    "low_conf_edge_case = Signal(ModelSignal.BINARY_ENTROPY_CONFIDENCE, \n",
    "                is_model_signal=True, extra_args={'conf_threshold': 0.8})\n",
    "\n",
    "cfg['checks'][0].update({\"signal_formulae\": (pushup_edge_case | low_conf_edge_case)})\n",
    "cfg['training_args'].update({'training_func': train_model_dnn})\n",
    "cfg['evaluation_args'].update({'inference_func': get_accuracy_dnn})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting the folder:  oodles_smart_data\n"
     ]
    }
   ],
   "source": [
    "framework_dnn = Framework(cfg)\n",
    "\n",
    "@monitor(framework_dnn)\n",
    "def model_predict(model, inputs):\n",
    "    with open('evaluation_logs.txt', 'w') as f:\n",
    "        with redirect_stdout(f):\n",
    "            return model.predict(inputs['data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50  edge-cases collected out of  188  inferred samples\n",
      "100  edge-cases collected out of  384  inferred samples\n",
      "150  edge-cases collected out of  593  inferred samples\n",
      "200  edge-cases collected out of  805  inferred samples\n",
      "250  edge-cases collected out of  964  inferred samples\n",
      "Kicking off re-training\n",
      "251 data-points selected out of 968\n",
      "Training on:  oodles_smart_data/1/training_dataset.json  which has  2255  data-points\n",
      "Epoch 1/10\n",
      "8/8 [==============================] - 1s 162ms/step - loss: 108.9199 - binary_accuracy: 0.3809\n",
      "Epoch 2/10\n",
      "8/8 [==============================] - 1s 162ms/step - loss: 72.8014 - binary_accuracy: 0.4009\n",
      "Epoch 3/10\n",
      "8/8 [==============================] - 1s 162ms/step - loss: 50.1117 - binary_accuracy: 0.6113\n",
      "Epoch 4/10\n",
      "8/8 [==============================] - 1s 160ms/step - loss: 45.2109 - binary_accuracy: 0.7466\n",
      "Epoch 5/10\n",
      "8/8 [==============================] - 1s 159ms/step - loss: 42.6730 - binary_accuracy: 0.7676\n",
      "Epoch 6/10\n",
      "8/8 [==============================] - 1s 172ms/step - loss: 37.1132 - binary_accuracy: 0.7188\n",
      "Epoch 7/10\n",
      "8/8 [==============================] - 1s 168ms/step - loss: 32.2595 - binary_accuracy: 0.6504\n",
      "Epoch 8/10\n",
      "8/8 [==============================] - 1s 167ms/step - loss: 29.8832 - binary_accuracy: 0.6987\n",
      "Epoch 9/10\n",
      "8/8 [==============================] - 1s 161ms/step - loss: 28.2743 - binary_accuracy: 0.7563\n",
      "Epoch 10/10\n",
      "8/8 [==============================] - 1s 162ms/step - loss: 25.2386 - binary_accuracy: 0.7393\n",
      "INFO:tensorflow:Assets written to: trained_models_dnn/version_1/assets\n",
      "Model saved at:  trained_models_dnn/version_1\n",
      "Model retraining done...\n",
      "Generating comparison report...\n",
      "Training on:  data/training_data.json  which has  1000  data-points\n",
      "Trained model exists. Skipping training again.\n",
      "Evaluating on  15731  data-points\n",
      "492/492 [==============================] - 0s 416us/step\n",
      "Evaluating on  15731  data-points\n",
      "492/492 [==============================] - 0s 406us/step\n",
      "---------------------------------------------\n",
      "---------------------------------------------\n",
      "Old model accuracy:  0.22382556735109022\n",
      "Retrained model accuracy (ie 251 smartly collected data-points added):  0.6579365583878966\n",
      "---------------------------------------------\n",
      "---------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "model_dir = 'trained_models_dnn/'\n",
    "model_save_name = 'version_0'\n",
    "real_world_dataset = KpsDataset(\n",
    "    real_world_test_cases, batch_size=inference_batch_size, shuffle=False, augmentations=False, is_test=True\n",
    ")\n",
    "model = tf.keras.models.load_model(model_dir + model_save_name)\n",
    "gt_data = read_json(annotation_args['master_file'])\n",
    "all_gt_ids = [x['id'] for x in gt_data]\n",
    "\n",
    "for i,elem in enumerate(real_world_dataset):\n",
    "\n",
    "    # Do model prediction\n",
    "    preds, idens = model_predict(model, {\"data\": elem[0][\"data\"], \"id\": elem[0][\"id\"]})\n",
    "\n",
    "    # Attach ground truth\n",
    "    this_elem_gt = [gt_data[all_gt_ids.index(x)]['gt'] for x in elem[0]['id']]\n",
    "    framework_dnn.attach_ground_truth({'id': idens, 'gt': np.array(this_elem_gt)})\n",
    "\n",
    "    # Retrain only once\n",
    "    if framework_dnn.version > 1:\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
