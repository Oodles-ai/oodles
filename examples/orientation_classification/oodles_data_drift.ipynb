{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we train a model using the orientation data of people during fitness exercises. The model tries to predict whether the person is in a vertical or a horizontal position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import subprocess\n",
    "import zipfile\n",
    "import numpy as np\n",
    "\n",
    "from oodles import Framework\n",
    "from oodles import Signal\n",
    "from oodles import monitor\n",
    "from oodles import ModelSignal, AnnotationMethod, Anomaly, DataDriftAlgo\n",
    "\n",
    "from dataset import input_to_dataset_transformation, read_json, write_json, KpsDataset\n",
    "from pushup_signal import pushup_signal, plot_all_cluster\n",
    "from contextlib import redirect_stdout\n",
    "\n",
    "import tensorflow as tf\n",
    "import joblib\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download dataset from remote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_dir = \"data\"\n",
    "remote_url = \"https://oodles-dev-training-data.s3.amazonaws.com/data.zip\"\n",
    "orig_training_file = 'data/training_data.json'\n",
    "if not os.path.exists(data_dir):\n",
    "    try:\n",
    "        file_downloaded_ok = subprocess.check_output(\"wget \" + remote_url, shell=True)\n",
    "    except:\n",
    "        print(\"Could not load training data\")\n",
    "    with zipfile.ZipFile(\"data.zip\", 'r') as zip_ref:\n",
    "        zip_ref.extractall(\"./\")\n",
    "\n",
    "    full_training_data = read_json(orig_training_file)\n",
    "    np.random.seed(1)\n",
    "    np.random.shuffle(full_training_data)\n",
    "    reduced_training_data = full_training_data[0:1000]\n",
    "    write_json(orig_training_file, reduced_training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_world_test_cases = 'data/real_world_testing_data.json'\n",
    "golden_testing_file = 'data/golden_testing_data.json'\n",
    "annotation_args = {'master_file': 'data/master_annotation_data.json'}\n",
    "\n",
    "# Defining the egde-case signal\n",
    "pushup_edge_case = Signal(\"Pushup\", pushup_signal)\n",
    "inference_batch_size = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we train our network using Deep Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on:  data/training_data.json  which has  1000  data-points\n",
      "Trained model exists. Skipping training again.\n"
     ]
    }
   ],
   "source": [
    "from model_dnn import get_accuracy_dnn, train_model_dnn\n",
    "train_model_dnn('data/training_data.json', 'version_0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we get the model accuracy on testing dataset, which is again low due to misclassification of Pushup signals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-13 14:14:07.494944: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating on  15731  data-points\n",
      "492/492 [==============================] - 0s 430us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.22382556735109022"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_accuracy_dnn(golden_testing_file, 'version_0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Update the Oodles config with new training workflows and checks. Let's also add a check for edge-cases when model confidence is low (because why not!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = {\n",
    "    # Define your signal to identify edge cases\n",
    "    \"checks\": [{\n",
    "        'type': Anomaly.DATA_DRIFT,\n",
    "        'reference_dataset': orig_training_file,\n",
    "        'cluster_plot_func': plot_all_cluster,\n",
    "    },\n",
    "    {\n",
    "        'type': Anomaly.CONCEPT_DRIFT,\n",
    "        'algorithm': DataDriftAlgo.DDM  \n",
    "    }],\n",
    "    \"data_identifier\": \"id\",\n",
    "    \"batch_size\": inference_batch_size,\n",
    "\n",
    "    # Connect training pipeline to annotate data and retrain the model\n",
    "    \"training_args\": {\n",
    "        \"data_transformation_func\": input_to_dataset_transformation,  \n",
    "        \"annotation_method\": {\"method\": AnnotationMethod.MASTER_FILE, \"args\": annotation_args}, \n",
    "        \"training_func\": train_model_dnn, \n",
    "        \"fold_name\": 'oodles_smart_data',  \n",
    "        \"orig_training_file\": orig_training_file,  \n",
    "    },\n",
    "\n",
    "    # Connect evaluation pipeline to test retrained model against original model\n",
    "    \"evaluation_args\": {\n",
    "        \"inference_func\": get_accuracy_dnn,\n",
    "        \"golden_testing_dataset\": golden_testing_file,\n",
    "        \"metrics_to_check\": ['accuracy']\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting the folder:  oodles_smart_data\n"
     ]
    }
   ],
   "source": [
    "framework_dnn = Framework(cfg)\n",
    "\n",
    "@monitor(framework_dnn)\n",
    "def model_predict(model, inputs):\n",
    "    with open('evaluation_logs.txt', 'w') as f:\n",
    "        with redirect_stdout(f):\n",
    "            return model.predict(inputs['data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50  edge-cases collected out of  1855  inferred samples\n",
      "100  edge-cases collected out of  3620  inferred samples\n",
      "150  edge-cases collected out of  5274  inferred samples\n",
      "200  edge-cases collected out of  6929  inferred samples\n",
      "250  edge-cases collected out of  8654  inferred samples\n",
      "Kicking off re-training\n",
      "251 data-points selected out of 8676\n",
      "Training on:  oodles_smart_data/1/training_dataset.json  which has  2255  data-points\n",
      "Trained model exists. Skipping training again.\n",
      "Model retraining done...\n",
      "Generating comparison report...\n",
      "Training on:  data/training_data.json  which has  1000  data-points\n",
      "Trained model exists. Skipping training again.\n",
      "Evaluating on  15731  data-points\n",
      "492/492 [==============================] - 0s 345us/step\n",
      "Evaluating on  15731  data-points\n",
      "492/492 [==============================] - 0s 351us/step\n",
      "---------------------------------------------\n",
      "---------------------------------------------\n",
      "Old model accuracy:  0.22382556735109022\n",
      "Retrained model accuracy (ie 251 smartly collected data-points added):  0.6579365583878966\n",
      "---------------------------------------------\n",
      "---------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "model_dir = 'trained_models_dnn/'\n",
    "model_save_name = 'version_0'\n",
    "real_world_dataset = KpsDataset(\n",
    "    real_world_test_cases, batch_size=inference_batch_size, shuffle=False, augmentations=False, is_test=True\n",
    ")\n",
    "model = tf.keras.models.load_model(model_dir + model_save_name)\n",
    "gt_data = read_json(annotation_args['master_file'])\n",
    "all_gt_ids = [x['id'] for x in gt_data]\n",
    "\n",
    "for i,elem in enumerate(real_world_dataset):\n",
    "\n",
    "    # Do model prediction\n",
    "    preds, idens = model_predict(model, {\"data\": elem[0][\"data\"], \"id\": elem[0][\"id\"]})\n",
    "\n",
    "    # Attach ground truth\n",
    "    this_elem_gt = [gt_data[all_gt_ids.index(x)]['gt'] for x in elem[0]['id']]\n",
    "    framework_dnn.attach_ground_truth({'id': idens, 'gt': np.array(this_elem_gt)})\n",
    "\n",
    "    # Retrain only once\n",
    "    if framework_dnn.version > 1:\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
